{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What questions we are trying to solve\n",
    "   - Business understanding, Clarify problem. What is the goal? (How to reduce the cost?).\n",
    "* How to use data to answer the question\n",
    "   - If question is to determine probability of action\n",
    "       - Use a predictive model\n",
    "   - Question show relationships\n",
    "       - Use descriptive model (Cluster of similar things)\n",
    "   - Yes/No anwer\n",
    "       - Classification model\n",
    "* What data do we need\n",
    "* Where is the data coming from and how will you get it\n",
    "* Is the data we collected is representative of the problem to be solved?\n",
    "    - Data understanding. Use descriptive statistics on data columns, Univariate statistics on each variable like mean, median, max, min, SD. Pairwise correlation between 2 variable and if 2 variable is highly correlated meaning one of the variable is redundent. \n",
    "    - Histogram to understand distribution.\n",
    "* What additional work needed on data?\n",
    "    - Missing values, invalid values. Remove duplicates\n",
    "    - Feature engineering: Using domain knowledge of the data to create features that make machine learning algorithms to work.\n",
    "* How data should be visualized\n",
    "* Does model used answer initial question or does it needs to be adjusted\n",
    "* Can we put model in practice\n",
    "* Can we get constructive feedback for answering problem?\n",
    "\n",
    "### Exploratory data analysis (EDA)\n",
    "* Examination of data and relationship between variable through numerical and graphical methods. First part of larger process. It lead to insight. Also checks for the bad data, we can check our intution and assumption about dataset.\n",
    "* Approach to understand data using visualization and statistical tools. we can examine distribution of variable using histogram, correlation between variable using scatter plot.\n",
    "* Which variable are normally distributed? \n",
    "* Be curious and skeptical.\n",
    "* Data set is not tidy sometimes so we have to reshape, transform data prior to EDA, which is called data munging.\n",
    "* EDA is an opportunity to let the data surprise you. Think what question you are trying to answer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis vs analytics\n",
    "* Huge dataset, instead of tackling entire at once, we split them study them individually and figure out how they are related. That is analysis. We perform analysis of thing which happened in past. How and why decrease in sell in last summer?\n",
    "* Analytics refers to future. Explore potential future events. Use knowledge which we obtained in analysis. \n",
    "    - Quantitative analytics: Use intuition and analysis to plan next business move\n",
    "    - Quantitative analytics : apply formula and algo to obtained analysis.\n",
    "    - Online cloth store, read articles on fashion and analysis it and we decide what to sell on store that is quantitative analytics. Using past data of sales we can decide when to launch new cloths that is part of quantitative analytics.\n",
    "* BI is process of analysing and reporting historical business data. Aims to explain past event using business data. BI dashboard  that can be used by end use to make business decision.\n",
    "    - Analyse past data and extract useful insights\n",
    "    - Create appropriate model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Process of using data to understand the world. \n",
    "* Data science is a field of exploring, analyzing, manipulating data to answer question and make recommendation.\n",
    "* Data scientist extracts insight from messy data. Using that insights make strategic choice for the company.\n",
    "* Business analytics can be defined as the broad use of data and quantitative analysis for decision making within organization. Get insight from huge amount of data.\n",
    "* Property of Big data:\n",
    "    - **Volume**\n",
    "    - **Variety**\n",
    "    - **Velocity**\n",
    "    - **Veracity**\n",
    "    - **Value**\n",
    "    \n",
    "# Types of Analysis\n",
    "\n",
    "### Diagnostic analysis\n",
    "* Something has happened but why?\n",
    "* One of the store did not make profit. Why?\n",
    "\n",
    "### Predictive analysis\n",
    "* What and when will happen?\n",
    "* Predicting demand.\n",
    "* Build a model using available data. Figuring out patterns/abnormality. \n",
    "\n",
    "### Prescriptive analysis\n",
    "* How can we make it happen?\n",
    "* Which medicine will work on patient.\n",
    "* How to optimize price?\n",
    "* How to spend marketing dollars?\n",
    "* How to decide the best route on Amazon truck?\n",
    "* How to do friend suggestion? Recommend jobs? Suggest a car? Advertise trip.\n",
    "* For oil production, how to recommend the most optimal fracking location?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data wrangling is about gathering the right pieces of data, assessing your data's quality and structure, then modifying your data to make it clean. But the assessments you make and convert to cleaning operations won't make your analysis, viz, or model better, though. The goal is to just make them possible, i.e., functional.\n",
    "\n",
    "* EDA is about exploring your data to later augment it to maximize the potential of our analyses, visualizations, and models. When exploring, simple visualizations are often used to summarize your data's main characteristics. From there you can do things like remove outliers and create new and more descriptive features from existing data, also known as feature engineering. Or detect and remove outliers so your model's fit is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps in Data Science\n",
    "### Question phase\n",
    "- Define problem/ question\n",
    "- How to detect failure from sensor data.\n",
    "- Characteristics of students who pass projects\n",
    "- How to stock stone?\n",
    "- Concern about cost and benefits, expected level of accuracy, usefulness of results.\n",
    "\n",
    "### Data acquisition/gathering\n",
    "* Output of data mining largely depends on quality of data.\n",
    "* Identify available data based on our need.\n",
    "* Data comes from many sources, variety (Structured, unstructured), velocity(real time streaming)\n",
    "* SQL to access data from traditional database.\n",
    "* Scripts are used to access data from Text, excel files.\n",
    "* Remote data from websites: Includes XML, JSON data. Many websites provides web services to provide programmatic access to their data. Ex. REST(representational state transfer), web socket (Allows real time notification from website), SOAP.\n",
    "* NO SQL storage: Cassandra, MongoDb, Hbase. It provides API to access data. Also provide web services such as REST.\n",
    "* Ex. In wild fire data analysis, historical data stored in relational database. Current weather, we access real time data with webSocket services. Real time tweets near fire using REST API from twitter.\n",
    "* Downloading dataset, import dataset, Accessing API, Scraping a web page\n",
    "* When we read file, make sure for delimiter, missing col labels, blank lines, comments, header text.\n",
    "\n",
    "### Exploration vs Explanation\n",
    "* Exploration is searching for insight\n",
    "* Explanation is highlighting insight. Surround by story.\n",
    "### Data preparation\n",
    "* Dirty data has lower quality or content issue.Ex. inaccurate duplicated data Messy data is untidy data, structural issue, not organized well. State is CA and California meaning there is no structure. This is part of data wrangling meaning finding issue with structure which make analysis hard. Explore the data is part of EDA.\n",
    "* Check data documentation well for missing value interpretation.\n",
    "* Data exploration and visualization: Getting familiar with data, understand nature of data build intuition, find patterns\n",
    "* Correlation (Explore dependency between different variables), general trends (How the data is progressing over the time), outliers (Help to double check the error or rare events), summary statistics (mean, median, mode, range). \n",
    "* Histogram shows distribution of data, skewness. Boxplot is used to see unusual observations, line chart is used to check changes over the time, scatter plot shows correlation between the variables.\n",
    "* Data cleaning : Deal with missing values, dealing with outliers\n",
    "    - Inconsistent column names\n",
    "    - Column types can signal unexpected data values\n",
    "    - Inconsistent values (customer with 2 different address, invalid zip code, outliers due to sensor failures).\n",
    "    - Duplicate records\n",
    "    - Missing values: Identify missing values are random or systematic.\n",
    "        - Particular group of people deny to disclose income, if we continue to use income for our analysis it will create bias in result.\n",
    "    - invalid data\n",
    "    - Not possible values (-ve heights) (more or less than 5 digit zipcode)\n",
    "    - Is data valid in current point of time?\n",
    "    - Incorrect data types\n",
    "    - Not consistent units (kg/lb)\n",
    "    - duplicates. `sum(df.duplicated())`\n",
    "    - We can merge duplicate records, remove data with missing values. For invalid values we can generate best estimates for invalid values. Outliers can be removed.\n",
    "    - discard irrelevant attributes of data.\n",
    "* Getting data in shape (Data munging, Data wrangling, Data preprocessing)\n",
    "    - Scaling (Changing range of values to specified range Ex. 0 and 1 so higher value does not dominate the result) \n",
    "    - transformation (aggregation, to avoid variability aggregate weekly sales data to monthly sales data. This may lose detail) Combine various income source as total income\n",
    "    - dimensionality reduction (Ex. PCA)\n",
    "    - feature selection (Remove feature, combining feature, creating new feature) We might say that one feature is very co-related to other, sales price and tax paid. so we can remove tax paid information.\n",
    "* Data normalization (Centering or scaling)\n",
    "    - Different columns may have different ranges, direct comparision is not possible, using normalization we can bring data in same range.\n",
    "    - For example age can rage between 0-120 and income can range between 5000-300000. They both are hard to compare, income will influence more in model (linear regression). We can convert both in 0 and 1 range. It will cause similar intrinsic influence on analytical model.\n",
    "    - **Simple feature scaling**: $X_{new} = \\frac{x_{old}}{x_{max}}$\n",
    "    - **Min-Max normalization** $x_{new} = \\frac{x_{old}-x_{min}}{x_{max}-x_{min}}$\n",
    "    - **Z-score** $x_{new} = \\frac{x_{new} - \\mu}{\\sigma}$\n",
    "* Data binning: Creating category from set of numerical values.\n",
    "* data integration from various source\n",
    "* Data reduction, data transformation, data subset\n",
    "* Before making cleaning always copy your data.\n",
    "\n",
    "### Data Analyze\n",
    "* Model selection\n",
    "    - Classification: Predict category of input data. Ex. Classify tumor\n",
    "    - Regression: Predict numeric value Ex. predict price of stock\n",
    "    - Clustering: Organize similar items in groups\n",
    "    - Graph analytics: Use graph structure to find connection between entities.\n",
    "    - Association Analysis: FInd rules to capture association between items (market basket analysis) (account with CD open credit cards) (Dyper beer connection)\n",
    "* Select technique -> Build model -> Evaluate model \n",
    "\n",
    "### Reporting insights\n",
    "* Present findings, Communicate results\n",
    "*   \n",
    "\n",
    "### Turning result in actions\n",
    "*  \n",
    "\n",
    "### Conclusion/ prediction\n",
    "* Gather training data -> Create features -> Train model, parameters -> Evaluate the model. \n",
    "- Predict what likely to happen\n",
    "- Which movie user will like\n",
    "- Involves statistics and machine learning\n",
    "- optimize solution, find best solution\n",
    "- Make decision and keep measuring the accuracy.\n",
    "\n",
    "### Communicate findings\n",
    "- blog post, paper, email, ppt, conversation\n",
    "- Data visualization\n",
    "\n",
    "\n",
    "### Big data infrastructure\n",
    "* Software that runs on 100s of machine and allows you to use these machines as a single big machine.\n",
    "* Hadoop, Spark, strom\n",
    "\n",
    "### Data mining\n",
    "* Given some data, can you find patterns in data. Search for frequent behavior\n",
    "* Market Basket example\n",
    "    - Put bread and butter together if they are bought together.\n",
    "    - Transactional dat ais useful, customer id, list of products bought and price\n",
    "    - Is soda typically purchased with banana?\n",
    "    - Beer - diaper bought together.\n",
    "* Such rules are called associated rules.\n",
    "* human genome data which cause cancer.\n",
    "* Outlier detection: abnormal case, finding rare events.\n",
    "\n",
    "### Machine learning / Deep learning\n",
    "* Making machine learn\n",
    "* Building predictive model\n",
    "* Deep learning is extension of neural network parts of machine learning\n",
    "* Supervised learning\n",
    "    - We have past data with labels. Eye images with label of glucoma is there or not.\n",
    "    - If label is number: regression\n",
    "    - If label is category yes/no, news types : classification\n",
    "        - spam detection\n",
    "    \n",
    "* Unsupervised learning\n",
    "    - Organize images in in groups like selfies, birthday , party etc.\n",
    "    - clustering \n",
    "        - Automatically discovering underlying structure of data. Ex. Market segmentation. High income group, low education/low income group etc. loyal customer vs new customers.\n",
    "* Reinforcement learning\n",
    "    - Learning with label (Label is given on demand)\n",
    "    - Robotics, learn to pick up object, after successfully finishing task we reward robot, if it fails then penalty. \n",
    "\n",
    "### Natural language processing\n",
    "* Processing on natural language.\n",
    "* Automatically create summary of 30 min speech.\n",
    "* Test processing, business card scan, speech recognition.\n",
    "* Spam detection\n",
    "* Part of speech tagging, tokenization, stemming, sentiment analysis, machine translation, information extraction, Question answering, paraphrase, summarization, dialog, speech recognition.\n",
    "\n",
    "### Information retrieval\n",
    "* From huge data retrieve useful data.\n",
    "\n",
    "### Web mining\n",
    "* Recommendation system\n",
    "    - linkedin, netflix, amazon, facebook, glassdoor.\n",
    "* Social network data mining: Twitter analytics\n",
    "* Advertisement on web\n",
    "* Mining structured information from web.\n",
    "\n",
    "### Social network analysis\n",
    "* Link prediction\n",
    "* Community detection\n",
    "* Influence propagation\n",
    "\n",
    "### Internet of things\n",
    "*\n",
    "\n",
    "### Visualization\n",
    "*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example OkCupid\n",
    "* Ok cupid have dataset of almost 300,000 questions, they clean up redundant, personal and obvious question and left with 50000 viable first date questions,.\n",
    "* After analyzing it they found correlation between having sex on first date to do you like taste of beer. \n",
    "* Long term couple vs liking of horror movie, traveled around another country alone?\n",
    "* Political group, ask do you prefer life simple or complex, if date answer simple she/he is conservative else liberal.\n",
    "* Being religious vs spelling and grammar issue.\n",
    "\n",
    "#### Example Walmart\n",
    "* Walmart labs analyze what is trending on social media, local events on sale, local weather effects on sales, What consumer buy in store and online.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web scrapping:\n",
    "* Crawling only gets info from page which are linked from seed pages.\n",
    "* What if we want info that is not linked? It involves fetching and extracting web pages. Ex. scrapping movie rating data that does not provide dump. Wikipedia scrapping for application like topic recognition. Scrapping labeled images for image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidy dataset\n",
    "* Each variable forms a column\n",
    "* Each observation forms a row\n",
    "* Same type of observation units forms a table.\n",
    "![](images/tidy_1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Table 3 is tidy version of table 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fixed variable should come first followed by measured each ordered so related are contiguous.\n",
    "* Untidy data    \n",
    "    - Column header are value not variable\n",
    "    - Multiple variable stores in one column\n",
    "    - Variable stored in both rows and cols\n",
    "    - Multiple type of observation stored in same table\n",
    "    - Single observation stored in multiple table.\n",
    "* Height and weight are good as variable name. If it is go under variable name it will create confusion.\n",
    "* home phone, cell phone vs phone and phone type\n",
    "* Functional relationship is easy to compare between col than rows.\n",
    "* Variable stored in row:\n",
    "    - Requires unstack, inverse of melt\n",
    "* 1 type in multiple table\n",
    "    - Combine all data by specific key\n",
    "* Most basic cleaning is metlting, string split and casting\n",
    "* Columns should have values not the variable\n",
    "    - Memory efficient and efficient in matrix operation\n",
    "![](images/prob1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* melt/unstack it to make it tidy.\n",
    "* Turn col into rows. wide data set to tall/long dataset\n",
    "* Melting is parameterized by list of col which is already a variable, other cols that needs to converted into variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/tidy-data-three.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/tidy-data-four.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality\n",
    "* **Completeness**: Do we have all records? Do we missing records? Are any specific rows, cols or cells missing?\n",
    "* **Validity**: We have the record but not valid. They do not conform to defined schema. Schema is defined set of rules. Ex No negative height.\n",
    "* **Accuracy**: Inaccurate data is wrong data that is valid.It adheres to defined schema but still incorrect. Ex. patient's weight is 5 pound greater because scale was faulty.\n",
    "* **Consistency**: Both valid and accurate, but multiple correct way to referring same thing. Ex. California, CA. Does all phone numbers in international format?\n",
    "* **Currency** : degree to which data is current with the world that it models. how up-to-date data is\n",
    "* **Conformance** : Does data items conforms to internal or external reference data? Does product code matches to catalog?\n",
    "* **Duplication** : Does duplicates exist in data?\n",
    "* **Integrity**:  Does all contacts belong to an account or are orphan?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dirty data \n",
    "* Data entry error\n",
    "* No data coding standard\n",
    "* Different schema is used for same data\n",
    "* Data do not have unique identifier\n",
    "* Data lost in transmission from one format to other\n",
    "* Corrupted sensor or storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Messy data comes from poor data planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Feature engineering is all about creating a new variable with a sum, difference, product, or ratio between those original variables that may lend a better insight into the research questions you seek to answer.\n",
    "* For example, if you have one variable that gives a count of crime incidents, and a second one that gives population totals, then you may want to engineer a new variable by dividing the former by the latter, obtaining an incident rate. This would account for a possible relationship between the original features where if there are more people, there might naturally be more chances for crimes to occur. If we looked at the raw counts rather than the incident rate, we risk just seeing information about population sizes rather than what we might really want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas vs SQL\n",
    "* Pandas is much easier for statistical analysis and visualization\n",
    "* easier to grab the data and slice/analyze/model data once I have it in pandas/python.\n",
    "* SQL string processing is horrible, lacks stddev, lacks plotting, No apply map."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
